# Задание 1

(Ссылка на схему TO-BE)[https://drive.google.com/file/d/1w_HHx9yQuDeYeHhIjKBZvKlRqDxlnr1s/view?usp=sharing]

## 1. Общая идея решения

1. **Три независимых региона** (A, Б, В) в разных частях РФ (например, Европа/Москва, Урал/Сибирь, Дальний Восток). В каждом регионе — собственная инфраструктура, чтобы обслуживать пользователей **локально** и тем самым снижать latency.

2. **Active-Active на уровне приложений**:

   - Во **всех** трёх регионах запущен полный набор микросервисов («InsureTech Kubernetes cluster»).
   - Пользователи из разных часовых поясов по балансировке направляются в ближайший регион.

3. **Stretched-кластер БД** на базе PostgreSQL Patroni:

   - Существует один общий кластер с тремя узлами (по одному в каждом регионе).
   - В любое время есть один Primary (для операций записи), реплики — Standby.
   - При сбое текущего Primary Patroni автоматически выполняет **failover** и назначает новый Primary в одном из оставшихся регионов (если имеется кворум).
   - Используется **pgBackRest** для резервного копирования и восстановления; **PMM** для мониторинга кластера

4. **Каждый регион** содержит:

   - **Kubernetes‑кластер** с микросервисами, масштабируемыми **горизонтально** (добавлением реплик Pod’ов).
   - **HAProxy** (DB Access) настраивается под Patroni (для записи / чтения).
   - **PostgreSQL Patroni** нода (участвует в общем «Stretched Patroni cluster»).

5. **Geo/DNS LoadBalancer** выполняет **географически-распределенную** маршрутизацию:

   - **CDN** отдает статический контент (HTML/JS/CSS) из ближайших узлов CDN.
   - Динамические запросы перенаправляются на «ближайший» регион (или любой доступный при сбоях).

6. **RTO/RPO** бизнес-требования (45 мин / 15 мин) достигаются за счёт:
   - Автоматического failover (Patroni) для кратчайшей реакции.
   - Регулярных резервных копий (WAL-архивирование) для гарантии восстановления за требуемое время.
   - Поддержания **99,9%** доступности, так как при сбое одного региона оставшиеся продолжают работать.

## 2. Ключевые технические аспекты

### 2.1 Горизонтальное масштабирование

- Проще и гибче, чем вертикальное:
  - При увеличении нагрузки достаточно добавить больше Pod’ов (реплик) в Kubernetes.
  - Балансировщики (LB) в регионе распределят новый объем запросов между Pod’ами.

### 2.2 Независимые кластеры Kubernetes

- Каждый регион имеет **свой** Kubernetes control plane и worker nodes.
- Не используется «растянутый» кластер K8s, чтобы избежать сетевых задержек, сложной конфигурации и потенциальных конфликтов.
- Сбой в одном кластере/регионе локализован — другие продолжают обслуживать трафик.

### 2.3 Active-Active failover стратегия

- На **уровне приложений**: все три региона способны принимать запросы одновременно.
- Повышает доступность и снижает время отклика: клиент идёт в «близкий» регион.
- Снимает «узкие горлышки», так как нагрузка распределяется.

### 2.4 Patroni и HAProxy

- **Patroni**:
  - Реализует консенсус (через etcd) и автоматический failover PostgreSQL.
  - Один Primary, остальные Standby. При сбое — быстрый failover.
- **HAProxy** в каждом регионе:
  - Получает состояние от Patroni (кто Primary) и перенаправляет **запись** на Primary, **чтение** — либо на Primary, либо на локальную реплику.

### 2.5 Обеспечение SLA: RTO, RPO, 99,9% доступности

- 99,9% достигается многорегиональной структурой: при падении (полном) одного региона заявки идут в другие.
- **RTO** (45 мин) — время, за которое инфраструктура «оправится» от серьёзного сбоя.
  - Фактически, при Patroni failover происходит быстрее (секунды/минуты).
  - Резервное восстановление (в случае серьёзных потерь) укладывается в 45 мин при подготовленной процедуре DR.
- **RPO** (15 мин) — гарантируется WAL-архивом и репликацией. При этом теряем не более 15 мин транзакций в худшем случае.

## 3. Итоговая схема (C4 Container Diagram)

Упрощённо выглядит так:

1. **Клиент**  
   ↓ (статические файлы → **CDN**)  
   ↓ (динамика) → **Geo LB / DNS**
2. **Geo LB** выбирает регион (A, Б, В).
3. **Регион A** (аналогично Б, В):
   - **LB A** → **Kubernetes (A)** → **(HAProxy A)** → **Patroni Node A**
4. **Регион Б**:
   - **LB B** → **Kubernetes (Б)** → **(HAProxy Б)** → **Patroni Node Б**
5. **Регион В**:
   - **LB C** → **Kubernetes (В)** → **(HAProxy В)** → **Patroni Node В**
6. Все три узла Patroni (A, Б, В) объединены в **один** кластер (Stretched cluster).

Каждый регион **полностью** может обслуживать входящие запросы. При сбое Primary (пускай он был в A) новый Primary может стать в Б или В.

## 4. Выгоды и соответствие бизнес-требованиям

1. **Масштабирование**:
   - Горизонтально (добавляем/убавляем Pod’ы в K8s) — эффективно и гибко.
2. **Универсальный Active-Active**:
   - Все регионы работают одновременно, клиенты обслуживаются с минимальными задержками.
3. **Сокращение рисков**:
   - Если регион «упал», пользователи автоматом переходят в другие.
   - Patroni быстро «перевыбирает» Primary для БД.
4. **Соответствие SLA**:
   - 99,9% доступности: многорегиональная архитектура.
   - RTO=45 мин / RPO=15 мин: резервное копирование + Patroni failover.
5. **Гибкий рост**:
   - При рекламных кампаниях можно быстро поднять больше Pod’ов и увеличить объём ресурсов.

Таким образом, данное решение **удовлетворяет** описанным техническим требованиям и бизнес-требованиям по отказоустойчивости, масштабированию и обслуживанию клиентов по всей территории РФ в режиме 24/7.
